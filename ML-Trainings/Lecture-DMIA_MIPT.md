## Немного для гуглинга
svn = l1(1 - M)
logReg = l2(1 - e^M)

libsvm - время работы НЕлинейный методов быстрее
liblinear - тоже самое, что  sklearn.linear_models.SVC(‘linear)
sklearn.linear_models - обертка
Vowpal Wabbit

## Композиция Алгоритмов

* бутстреп (для оценки зависимостей). Разбиваем выборку на подвыборки с возвращением

* bagging - bootstrap aggregation. Вариации:

* RSM - Random Subspace Method - выбираем объекты, а не признаки

* Parsing? - выбираем объекты без возвращения

* Stacking - обучение одних алгоритмов на выходе других алгоритмов (градиентный бустинг поверх других алгоритмов). **ОЧЕНЬ КРУТАЯ ШТУКА**

* **Blending** - подбираем веса парочки крутых алгоритмов)))

* **Boosting**  - пробуем делать алгоритм на основед других более легких алгоритмов - жадно. AdaBoost. Gradient Boosting.


## Семинар

* Посмотреть на sigmiod - предсказываем разницу в счете, закидываем ее в сигмойду, обрубаем конци и считаем вероятности

* Данные https://www.kaggle.com/c/march-machine-learning-mania-2016
